# Результаты обучения 4-битного XOR (проверка четности)

## Результаты

```
Starting 4-bit XOR training (parity check)...
Epoch 0, Loss: 0.269529
Epoch 500, Loss: 0.000303056
Epoch 1000, Loss: 6.36798e-05
Epoch 1500, Loss: 2.71521e-05
Epoch 2000, Loss: 1.4685e-05
Epoch 2500, Loss: 8.94016e-06
Epoch 3000, Loss: 5.83393e-06
Epoch 3500, Loss: 3.98057e-06
Epoch 4000, Loss: 2.79962e-06
Epoch 4500, Loss: 2.01178e-06
Epoch 5000, Loss: 1.46834e-06

Training complete. Testing...

Input (A B C D) | Sum | Expected | Prediction
-----------------------------------------------
0 0 0 0 | 0 | 0 | 0.000108492 (0)
0 0 0 1 | 1 | 1 | 0.999747 (1)
0 0 1 0 | 1 | 1 | 0.99915 (1)
0 0 1 1 | 2 | 0 | 0.00168727 (0)
0 1 0 0 | 1 | 1 | 0.998891 (1)
0 1 0 1 | 2 | 0 | 4.63838e-05 (0)
0 1 1 0 | 2 | 0 | 0.00117962 (0)
0 1 1 1 | 3 | 1 | 0.998517 (1)
1 0 0 0 | 1 | 1 | 0.998904 (1)
1 0 0 1 | 2 | 0 | 0.00105871 (0)
1 0 1 0 | 2 | 0 | 0.000671347 (0)
1 0 1 1 | 3 | 1 | 0.99844 (1)
1 1 0 0 | 2 | 0 | 0.00128799 (0)
1 1 0 1 | 3 | 1 | 0.998905 (1)
1 1 1 0 | 3 | 1 | 0.998208 (1)
1 1 1 1 | 4 | 0 | 0.00193276 (0)
```

## Метрики производительности

- **Начальная ошибка**: 0.269529
- **Финальная ошибка**: 1.46834e-06 (снижение в ~183,000 раз)
- **Точность**: 16/16 (100%)
- **Эпох обучения**: 5000
- **Скорость обучения**: 0.01
- **Архитектура**: 4 → 32 (tanh) → 1 (sigmoid)

## Почему это работает

### 1. **Сложность задачи**
Проверка четности 4-битного входа — нелинейная задача классификации, требующая изучения XOR-зависимостей между всеми входными битами. В отличие от линейных моделей, нейронная сеть с нелинейными активациями может выучить этот паттерн.

### 2. **Выбор архитектуры**
- **Скрытый слой (32 нейрона)**: Обеспечивает достаточную емкость для изучения сложных взаимодействий между битами
- **Активация Tanh**: Ограниченная, симметричная активация, способствующая потоку градиентов при обратном распространении
- **Выход Sigmoid**: Преобразует логиты в вероятностное пространство [0,1] для бинарной классификации

### 3. **Динамика обучения**
- **Оптимизатор Adam**: Адаптивная скорость обучения помогает сходиться быстрее, чем SGD
- **Функция потерь MSE**: Подходит для задачи регрессии к вероятности
- **Обучение на полном батче**: Использование всех 16 образцов на эпоху обеспечивает стабильные оценки градиента

### 4. **Поток градиентов**
Движок автоматического дифференцирования корректно вычисляет градиенты через:
- Матричное умножение (Dense слои)
- Нелинейные активации (tanh, sigmoid)
- Вычисление потерь (MSE)
- Операции broadcasting (добавление bias)

### 5. **Численная стабильность**
- Инициализация Xavier предотвращает исчезающие/взрывающиеся градиенты
- Выход Sigmoid предотвращает экстремальные предсказания
- Потери сходятся плавно без колебаний

## Ключевые детали реализации

1. **Поддержка Broadcasting**: Операция `add` обрабатывает broadcasting bias от `{1, N}` к `{M, N}`, критично для пакетной обработки
2. **Накопление градиентов**: Обратный проход корректно суммирует градиенты по размерности батча для broadcasted операций
3. **Вычислительный граф**: Динамическое построение графа позволяет гибкие forward/backward проходы

## Заключение

Модель успешно изучает функцию четности, обнаруживая нелинейные паттерны в 4-битном входном пространстве. Комбинация достаточной емкости (32 скрытых нейрона), подходящих активаций и стабильной оптимизации обеспечивает идеальную классификацию всех 16 входных комбинаций.
